{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing baseline neural networks defined in this paper `https://arxiv.org/pdf/1611.06455.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smokingml.modules import test, train, validate_on_holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_SIZE = 101\n",
    "\n",
    "class SmokingDataset(Dataset):\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.dir))\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            stop, start, step = key.indices(len(self))\n",
    "\n",
    "            length = len(range(stop, start, step))\n",
    "            X = torch.zeros([length, 3*WIN_SIZE])\n",
    "            y = torch.zeros([length, 1])\n",
    "\n",
    "            for j,i in enumerate(range(stop, start, step)):\n",
    "                xi, yi = self[i]\n",
    "                X[j] = xi\n",
    "                y[j] = yi\n",
    "\n",
    "            return (X, y)\n",
    "\n",
    "        elif isinstance(key, int):\n",
    "            X, y = torch.load(os.path.join(self.dir, f'{key}.pt'))\n",
    "            return (X.flatten(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../data/working-dataset-20Hz/'\n",
    "train_dataset = SmokingDataset(f'{dir}/4_all/train/')\n",
    "test_dataset = SmokingDataset(f'{dir}/4_all/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "Three hidden layer MLP, with 500 neurons each\n",
    "\n",
    "Dropout after each layer with probabilites:\n",
    "- 0.1 after input layer\n",
    "- 0.2 between hidden layers\n",
    "- 0.3 before output neuron\n",
    "\n",
    "\n",
    "one output neuron to predict smoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "nhl = 500\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First Hidden Layer\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.h1 = nn.Linear(in_features=WIN_SIZE*3, out_features=nhl)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Second Hidden Layer\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.h2 = nn.Linear(in_features=nhl, out_features=nhl)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Third Hidden Layer\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "        self.h3 = nn.Linear(in_features=nhl, out_features=nhl)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Output Layer\n",
    "        self.dropout4 = nn.Dropout(p=0.3)\n",
    "        self.h4 = nn.Linear(in_features=nhl, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Hidden Layer\n",
    "        x = self.dropout1(x)\n",
    "        x = self.h1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # Second Hidden Layer\n",
    "        x = self.dropout2(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # Third Hidden Layer\n",
    "        x = self.dropout3(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # Output Layer\n",
    "        x = self.dropout4(x)\n",
    "        logits = self.h4(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = MLP().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1, rho=0.95, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 64\n",
    "\n",
    "train(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    test_batch_size=10000,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    date='',\n",
    "    device=device,\n",
    "    project='.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/model-epoch-22.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mStarting validate_on_holdouts\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./results/holdouts/60’: File exists\n",
      "mkdir: cannot create directory ‘./results/holdouts/57’: File exists\n",
      "mkdir: cannot create directory ‘./results/holdouts/53’: File exists\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Evaluate on holdouts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m validate_on_holdouts(\n\u001b[1;32m      3\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      4\u001b[0m     holdout_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mdir\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/holdouts\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     df_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mdir\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/1_xyz\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     raw_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mdir\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/0_raw\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     date\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     win_size\u001b[39m=\u001b[39;49mWIN_SIZE,\n\u001b[1;32m     11\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     12\u001b[0m     project\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     dm_factor\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/smokingml/utils/print_on_start_and_end.py:9\u001b[0m, in \u001b[0;36mprint_on_start_and_end.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mColors\u001b[39m.\u001b[39mOKGREEN\u001b[39m}\u001b[39;00m\u001b[39mStarting \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mColors\u001b[39m.\u001b[39mENDC\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     10\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mColors\u001b[39m.\u001b[39mWARNING\u001b[39m}\u001b[39;00m\u001b[39mFinished \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. Elapsed time: \u001b[39m\u001b[39m{\u001b[39;00mend\u001b[39m-\u001b[39mstart\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mColors\u001b[39m.\u001b[39mENDC\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/smokingml/modules/validate_on_holdouts.py:75\u001b[0m, in \u001b[0;36mvalidate_on_holdouts\u001b[0;34m(model, holdout_dir, df_dir, raw_dir, date, criterion, batch_size, win_size, device, project, dm_factor, cnn)\u001b[0m\n\u001b[1;32m     72\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[39m# Test loop\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m     76\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device) \n\u001b[1;32m     77\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate on holdouts\n",
    "validate_on_holdouts(\n",
    "    model=model,\n",
    "    holdout_dir=f'{dir}/holdouts',\n",
    "    df_dir=f'{dir}/1_xyz',\n",
    "    raw_dir=f'{dir}/0_raw',\n",
    "    date='',\n",
    "    criterion=criterion,\n",
    "    batch_size=10000,\n",
    "    win_size=WIN_SIZE,\n",
    "    device=device,\n",
    "    project='.',\n",
    "    dm_factor=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Holdouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN\n",
    "Fully Convolutional Network\n",
    "All convolution with 1D kernels, no stride\n",
    "\n",
    "Three convolution blocks, all following by batch norm and relu:\n",
    "1. 128 kernels, kernel size 8\n",
    "2. 256 kernels, kernel size 5\n",
    "3. 128 kernels, kernel size 3\n",
    "\n",
    "Final global pooling layer before output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X to contain 3 channels (shape 3x101 instead of 303)\n",
    "\n",
    "WIN_SIZE = 101\n",
    "\n",
    "class SmokingDatasetCNN(Dataset):\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.dir))\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            stop, start, step = key.indices(len(self))\n",
    "\n",
    "            length = len(range(stop, start, step))\n",
    "            X = torch.zeros([length, 3, WIN_SIZE])\n",
    "            y = torch.zeros([length, 1])\n",
    "\n",
    "            for j,i in enumerate(range(stop, start, step)):\n",
    "                xi, yi = self[i]\n",
    "                X[j] = xi.reshape(3, WIN_SIZE)\n",
    "                y[j] = yi\n",
    "\n",
    "            return (X, y)\n",
    "\n",
    "        elif isinstance(key, int):\n",
    "            X, y = torch.load(os.path.join(self.dir, f'{key}.pt'))\n",
    "            X = X.reshape([3, WIN_SIZE])\n",
    "            return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SmokingDatasetCNN(f'{dir}/4_all/train')\n",
    "test_dataset = SmokingDatasetCNN(f'{dir}/4_all/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First Convolution Block\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=128, kernel_size=8)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.gp = lambda x: torch.mean(x, dim=2)    # Take mean across each feature map (N, C, L) => (N,C)\n",
    "        \n",
    "        # Output Later\n",
    "        self.output = nn.Linear(in_features=128, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.gp(x)\n",
    "        logits = self.output(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = FCN().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mStarting train\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: Testing: 100%|██████████| 40/40 [12:21<00:00, 18.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mFinished train. Elapsed time: 741.856\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 64\n",
    "\n",
    "train(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    test_batch_size=10000,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    date='',\n",
    "    device=device,\n",
    "    project='.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/model-epoch-29.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mStarting validate_on_holdouts\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./results/holdouts/60’: File exists\n",
      "mkdir: cannot create directory ‘./results/holdouts/57’: File exists\n",
      "mkdir: cannot create directory ‘./results/holdouts/53’: File exists\n",
      "mkdir: cannot create directory ‘./results/holdouts/8’: File exists\n",
      "mkdir: cannot create directory ‘./results/holdouts/31’: File exists\n",
      "mkdir: cannot create directory ‘./results/holdouts/27’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────┬──────────┬──────────┬──────────┬──────────┬──────────┬──────────┐\n",
      "│          │        8 │       27 │       31 │       53 │       57 │       60 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│ Accuracy │ 0.927887 │ 0.937633 │ 0.952751 │ 0.901715 │ 0.973956 │ 0.975031 │\n",
      "├──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┤\n",
      "│ Loss     │ 1.79284  │ 0.502777 │ 0.847194 │ 0.483618 │ 1.14741  │ 0.298814 │\n",
      "└──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┘\n",
      "\u001b[93mFinished validate_on_holdouts. Elapsed time: 6.392\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "validate_on_holdouts(\n",
    "    model=model,\n",
    "    holdout_dir=f'{dir}/holdouts',\n",
    "    df_dir=f'{dir}/1_xyz',\n",
    "    raw_dir=f'{dir}/0_raw',\n",
    "    date='',\n",
    "    criterion=criterion,\n",
    "    batch_size=10000,\n",
    "    win_size=WIN_SIZE,\n",
    "    device=device,\n",
    "    project='.',\n",
    "    dm_factor=5,\n",
    "    cnn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "Adds shortcut connection in ecah residual block\n",
    "Three residual blocks. Each consist of three convolutional blocks \n",
    "- conv blocks are same as before of 64, 128, and 128 kernels each\n",
    "    - conv kernel size goes 8->5->3 for each residual block\n",
    "- final output of residual block is output of third convolutional block + input to residual block\n",
    "\n",
    "output of third residual block passed to a global pooling layer before a linear layer to a single output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def conv_block(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(num_features=out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def res_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_block(in_channels=in_channels, out_channels=out_channels, kernel_size=8),\n",
    "            self.conv_block(in_channels=out_channels, out_channels=out_channels, kernel_size=5),\n",
    "            self.conv_block(in_channels=out_channels, out_channels=out_channels, kernel_size=3)\n",
    "        )\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.res1 = self.res_block(in_channels=3, out_channels=64)\n",
    "        self.res2 = self.res_block(in_channels=64, out_channels=128)\n",
    "        self.res3 = self.res_block(in_channels=128, out_channels=128)\n",
    "        self.gp = lambda x: torch.mean(x, dim=2)    # Take mean across each feature map (N, C, L) => (N,C)\n",
    "        self.output = nn.Linear(in_features=128, out_features=1)\n",
    "\n",
    "    def forward(x):\n",
    "        pass\n",
    "\n",
    "model = ResNet().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 94 elements not 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x,y \u001b[39m=\u001b[39m test_dataset[\u001b[39m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m x,y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mres1(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 94 elements not 64"
     ]
    }
   ],
   "source": [
    "x,y = test_dataset[0]\n",
    "x,y = x.to(device), y.to(device)\n",
    "model.res1(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
