{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoking_data/smoking_input.csv\n",
      "smoking_data/smoking_targets.csv\n"
     ]
    }
   ],
   "source": [
    "# Get raw, windowed data\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists('./data/raw/smoking_input.csv') or not os.path.exists('./data/raw/smoking_targets.csv'):\n",
    "    os.system('mkdir -p data/raw')\n",
    "    urllib.request.urlretrieve(\"http://ifestos.cse.sc.edu/datasets/smoking_data.tar.gz\", \"data/smoking_data.tar.gz\")\n",
    "\n",
    "    os.system('tar -xzvf data/smoking_data.tar.gz -C data/raw/ --strip-components=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X = pd.read_csv('data/raw/smoking_input.csv', header=None)\n",
    "y = pd.read_csv('data/raw/smoking_targets.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting & Preprocessing\n",
    "\n",
    "y.columns = [\"label\"]\n",
    "\n",
    "# Fill NaNs\n",
    "X = X.fillna(method='bfill')\n",
    "\n",
    "# Put in torch tensors\n",
    "X_pt = torch.from_numpy(X.to_numpy()).float().to(device)\n",
    "y_pt = torch.from_numpy(y.to_numpy()).float().to(device)\n",
    "\n",
    "# train-test split\n",
    "\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X_pt, y_pt, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Model\n",
    "- input layer 300 -> 10\n",
    "- activation function ReLU\n",
    "- hidden layer 10 -> 1\n",
    "- output function Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, batch_size, loss_fn, optimizer, X_train, X_test, y_train, y_test):\n",
    "    losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        for j in range(0, len(X_train), batch_size):\n",
    "            end = j+batch_size if j+batch_size < len(X_train) else len(X_train)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(X_train[j:end])\n",
    "            loss = loss_fn(logits, y_train[j:end])\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[i] += loss\n",
    "\n",
    "        losses[i] /= batch_size # get mean loss of all batches this epoch\n",
    "\n",
    "        test_logits = model(X_test)\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_losses[i] = test_loss\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f\"Epoch {i}: \", end='')\n",
    "            print(f'\\tLoss={loss.item()}', end='')\n",
    "            print(f'\\tTest Loss={test_loss.item()}')\n",
    "\n",
    "    return (losses, test_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01104\n",
      "Accuracy: 99.76%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "\n",
    "    loss = loss_fn(logits, y_test)\n",
    "\n",
    "    pred = torch.round(nn.Sigmoid()(logits))\n",
    "    accuracy = (sum(y_test == pred) / len(y_test)).item()\n",
    "\n",
    "print(f'Test Loss: {loss:.4}')\n",
    "print(f'Accuracy: {100*accuracy:.4}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "os.system('mkdir model')\n",
    "torch.save(model.state_dict(), 'model/model.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Continous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "\n",
    "raw = pd.read_csv(f'data/{file_index}/raw_data.csv', header=None)\n",
    "\n",
    "# Window Data\n",
    "df = raw[[2,3,4]]\n",
    "\n",
    "X = np.empty((len(df)-99, 300), dtype=float)\n",
    "for i in range(len(df)-99):\n",
    "    X[i] = df[i:i+100].to_numpy().T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "annot = {}\n",
    "y = np.zeros((len(X)))\n",
    "\n",
    "\n",
    "with open(f'data/{file_index}/16_data.json') as f:\n",
    "    annot = json.load(f)\n",
    "\n",
    "for i in range(annot['start'], annot['end']):\n",
    "    for puff in annot['puffs']:\n",
    "        if i >= puff['start'] and i <= puff['end'] - 99:\n",
    "            y[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2069270/1490788147.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = np.pad(y*10, (0,99), mode='constant', constant_values=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 16:59:29.738: Failed to load module \"xapp-gtk3-module\"\n",
      "Gtk-Message: 16:59:29.738: Failed to load module \"appmenu-gtk-module\"\n"
     ]
    }
   ],
   "source": [
    "# visualize true labels\n",
    "\n",
    "df['labels'] = np.pad(y*10, (0,99), mode='constant', constant_values=5)\n",
    "\n",
    "fig = px.line(data_frame=df.loc[::10,[2, \"labels\"]])\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.13%\n"
     ]
    }
   ],
   "source": [
    "# Predict labels\n",
    "\n",
    "X_pt = torch.from_numpy(X).float().to(device)\n",
    "y_pt = torch.from_numpy(y).reshape(-1,1).float().to(device)\n",
    "\n",
    "model = MLP().to(device)\n",
    "model.load_state_dict(torch.load('model/model.pt'))\n",
    "model.eval()\n",
    "\n",
    "step = 10000    # for memory\n",
    "preds = []\n",
    "correct = 0\n",
    "for i in range(0, len(X_pt), step):\n",
    "    end = i+step if i+step < len(X_pt) else len(X_pt)\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_pt[i:end])\n",
    "        pred = torch.round(nn.Sigmoid()(logits))\n",
    "        correct += sum(y_pt[i:end] == pred)\n",
    "        preds += pred.flatten().tolist()\n",
    "\n",
    "preds = np.array(preds)\n",
    "accuracy = (correct / len(y_pt)).item()\n",
    "print(f'Accuracy: {100*accuracy:.4}%')  # falsely high bc of all the true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in Smoking Session: 73.51%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy based on just the smoking session windows\n",
    "X_smoking = X[annot['start']:annot['end']]\n",
    "preds_smoking = preds[annot['start']:annot['end']]\n",
    "\n",
    "acc_smoking = sum(preds_smoking == y[annot['start']:annot['end']]) / len(preds_smoking)\n",
    "\n",
    "print(f'Accuracy in Smoking Session: {100*acc_smoking:.4}%')  # still high bc of the true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of positives predicted: 0.2674%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy by true positives\n",
    "\n",
    "tps = 0\n",
    "for i in range(len(y)):\n",
    "    tps += (y[i] and preds[i])  # add one if both are 1\n",
    "\n",
    "# acc_tp = len(np.where(preds==1)[0]) / len(np.where(y==1)[0])\n",
    "acc_tp = tps / len(np.where(y==1)[0])\n",
    "print(f'Percent of positives predicted: {100*acc_tp:.4}%')  # oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2069270/2576325511.py:3: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 17:10:39.092: Failed to load module \"xapp-gtk3-module\"\n",
      "Gtk-Message: 17:10:39.092: Failed to load module \"appmenu-gtk-module\"\n"
     ]
    }
   ],
   "source": [
    "# Visualize predictions\n",
    "\n",
    "df['preds'] = np.pad(preds*10, (0, 99), mode='constant', constant_values=0)\n",
    "\n",
    "figure = px.line(df.loc[::10, [2, 'labels', 'preds']])\n",
    "figure.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \tLoss=0.16119280457496643\tTest Loss=0.12817129492759705\n",
      "Epoch 10: \tLoss=0.12037324160337448\tTest Loss=0.05572707951068878\n",
      "Epoch 20: \tLoss=0.07652546465396881\tTest Loss=0.05539275333285332\n",
      "Epoch 30: \tLoss=0.05702270567417145\tTest Loss=0.05460852384567261\n",
      "Epoch 40: \tLoss=0.05092347413301468\tTest Loss=0.05482245609164238\n",
      "Epoch 50: \tLoss=0.046775057911872864\tTest Loss=0.05532114580273628\n",
      "Epoch 60: \tLoss=0.04248373955488205\tTest Loss=0.05540613457560539\n",
      "Epoch 70: \tLoss=0.03857682645320892\tTest Loss=0.05524738132953644\n",
      "Epoch 80: \tLoss=0.03610120713710785\tTest Loss=0.055296171456575394\n",
      "Epoch 90: \tLoss=0.03445607051253319\tTest Loss=0.05581223592162132\n",
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 17:32:16.566: Failed to load module \"xapp-gtk3-module\"\n",
      "Gtk-Message: 17:32:16.566: Failed to load module \"appmenu-gtk-module\"\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = MLP().to(device)\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X_pt[annot['start']:annot['end']], y_pt[annot['start']:annot['end']], test_size=0.25, stratify=y_pt[annot['start']:annot['end']].to('cpu'))\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "(losses, test_losses) = train(model, epochs, batch_size, loss_fn, optimizer, X_train, X_test, y_train, y_test)\n",
    "\n",
    "figure = px.line(pd.DataFrame({\"loss\": losses, \"validation loss\":test_losses}))\n",
    "figure.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m pred \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSigmoid()(logits)\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m----> 7\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(y_test\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), pred\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), normalize\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m sns\u001b[39m.\u001b[39mheatmap(cm, annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[1;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    234\u001b[0m ):\n\u001b[1;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:88\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m     87\u001b[0m type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my_pred\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     90\u001b[0m y_type \u001b[39m=\u001b[39m {type_true, type_pred}\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/multiclass.py:298\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be class \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseSeries\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseArray\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 298\u001b[0m \u001b[39mif\u001b[39;00m is_multilabel(y):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m \u001b[39m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/multiclass.py:161\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    159\u001b[0m warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mVisibleDeprecationWarning)\n\u001b[1;32m    160\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     y \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39;49masarray(y)\n\u001b[1;32m    162\u001b[0m \u001b[39mexcept\u001b[39;00m (np\u001b[39m.\u001b[39mVisibleDeprecationWarning, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    163\u001b[0m     \u001b[39m# dtype=object should be provided explicitly for ragged arrays,\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# see NEP 34\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     y \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_array_api.py:75\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.asarray\u001b[0;34m(self, x, dtype, device, copy)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy\u001b[39m.\u001b[39marray(x, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy\u001b[39m.\u001b[39;49masarray(x, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:956\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 956\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "logits = model(X_test)\n",
    "pred = nn.Sigmoid()(logits)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test.detach().to('cpu'), pred.detach().to('cpu'), normalize=\"true\")\n",
    "sns.heatmap(cm, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
